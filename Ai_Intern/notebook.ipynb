{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Knowledge Helper - RAG System Implementation\n",
        "\n",
        "This notebook provides a step-by-step implementation of a Retrieval-Augmented Generation (RAG) question-answering system.\n",
        "\n",
        "## Overview\n",
        "1. **Data Ingestion**: Extract text from PDF files\n",
        "2. **Preprocessing**: Clean and chunk documents\n",
        "3. **Embeddings**: Generate semantic embeddings\n",
        "4. **Vector Database**: Store and search embeddings\n",
        "5. **RAG Pipeline**: Retrieve + LLM reasoning\n",
        "6. **Evaluation**: Assess retrieval quality\n",
        "7. **ML Components**: Summarization and question classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add current directory to path\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import project modules\n",
        "from data_ingestion import ingest_pdfs\n",
        "from preprocessing import preprocess_text\n",
        "from embeddings import create_embeddings, create_query_embedding\n",
        "from retrieval import setup_vector_db, load_vector_db, search_similar\n",
        "from qa_pipeline import RAGPipeline\n",
        "from evaluation import comprehensive_evaluation\n",
        "from ml_components import summarize_chunks, classify_question_type\n",
        "import config\n",
        "\n",
        "print(\"All modules imported successfully!\")\n",
        "print(f\"Embedding model: {config.EMBEDDING_MODEL}\")\n",
        "print(f\"Chunk size: {config.CHUNK_SIZE} tokens\")\n",
        "print(f\"Top K retrieval: {config.TOP_K}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Data Ingestion\n",
        "\n",
        "First, we'll extract text from PDF files in the `data/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Ingest PDF files\n",
        "data_dir = \"data\"\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "Path(data_dir).mkdir(exist_ok=True)\n",
        "\n",
        "# Ingest PDFs\n",
        "texts = ingest_pdfs(data_dir)\n",
        "\n",
        "print(f\"\\nIngested {len(texts)} PDF documents\")\n",
        "\n",
        "# Display sample text from first document\n",
        "if texts:\n",
        "    print(f\"\\nFirst document preview ({len(texts[0])} characters):\")\n",
        "    print(texts[0][:500] + \"...\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No PDF files found in the data directory.\")\n",
        "    print(\"Please add PDF files to the 'data/' directory and re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Text Preprocessing\n",
        "\n",
        "Clean and chunk the extracted text for optimal retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Preprocess and chunk text\n",
        "if texts:\n",
        "    chunks = preprocess_text(\n",
        "        texts,\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP\n",
        "    )\n",
        "    \n",
        "    print(f\"Created {len(chunks)} text chunks\")\n",
        "    print(f\"\\nSample chunk (first 200 characters):\")\n",
        "    print(chunks[0][:200] + \"...\" if len(chunks) > 0 else \"No chunks created\")\n",
        "    \n",
        "    # Display chunk statistics\n",
        "    from preprocessing import count_tokens\n",
        "    chunk_sizes = [count_tokens(chunk) for chunk in chunks[:10]]\n",
        "    print(f\"\\nAverage chunk size (first 10): {sum(chunk_sizes) / len(chunk_sizes):.1f} tokens\")\n",
        "else:\n",
        "    print(\"No texts to preprocess. Please run the data ingestion step first.\")\n",
        "    chunks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Generate Embeddings\n",
        "\n",
        "Create semantic embeddings using SentenceTransformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Create embeddings\n",
        "if chunks:\n",
        "    print(\"Generating embeddings (this may take a few minutes)...\")\n",
        "    embeddings, chunks = create_embeddings(chunks)\n",
        "    \n",
        "    print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
        "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "    \n",
        "    # Display embedding statistics\n",
        "    print(f\"\\nEmbedding statistics:\")\n",
        "    print(f\"  Mean: {embeddings.mean():.4f}\")\n",
        "    print(f\"  Std: {embeddings.std():.4f}\")\n",
        "    print(f\"  Min: {embeddings.min():.4f}\")\n",
        "    print(f\"  Max: {embeddings.max():.4f}\")\n",
        "else:\n",
        "    print(\"No chunks to embed. Please run preprocessing step first.\")\n",
        "    embeddings = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Vector Database Setup\n",
        "\n",
        "Store embeddings in ChromaDB for efficient similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Setup vector database\n",
        "if embeddings is not None and chunks:\n",
        "    print(\"Setting up vector database...\")\n",
        "    collection = setup_vector_db(embeddings, chunks)\n",
        "    \n",
        "    print(f\"✅ Vector database created successfully!\")\n",
        "    print(f\"   Location: {config.VECTOR_DB_PATH}\")\n",
        "    print(f\"   Collection: {config.COLLECTION_NAME}\")\n",
        "    print(f\"   Documents stored: {len(chunks)}\")\n",
        "    \n",
        "    # Test retrieval\n",
        "    print(\"\\nTesting retrieval with sample query...\")\n",
        "    test_query = \"What is the main topic?\"\n",
        "    query_emb = create_query_embedding(test_query)\n",
        "    results = search_similar(query_emb, top_k=2)\n",
        "    \n",
        "    print(f\"\\nQuery: '{test_query}'\")\n",
        "    print(f\"Retrieved {len(results)} documents:\")\n",
        "    for i, (doc, score) in enumerate(results, 1):\n",
        "        print(f\"\\n  {i}. Similarity: {score:.3f}\")\n",
        "        print(f\"     Text: {doc[:150]}...\")\n",
        "else:\n",
        "    print(\"No embeddings available. Please run previous steps first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: RAG Pipeline - Question Answering\n",
        "\n",
        "Use the complete RAG pipeline to answer questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Initialize RAG pipeline\n",
        "rag = RAGPipeline()\n",
        "\n",
        "# Check if vector DB is loaded\n",
        "if not rag.vector_db_loaded:\n",
        "    print(\"Vector database not found. Processing documents...\")\n",
        "    rag.process_documents(data_dir)\n",
        "else:\n",
        "    print(\"✅ Vector database loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question\n",
        "question = \"What is the main topic of the document?\"\n",
        "\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(\"Processing...\\n\")\n",
        "\n",
        "result = rag.ask(question, use_summarization=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ANSWER:\")\n",
        "print(\"=\" * 60)\n",
        "print(result[\"answer\"])\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"METADATA:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "print(f\"Number of sources: {result['num_sources']}\")\n",
        "print(f\"\\nTop sources:\")\n",
        "for i, source in enumerate(result['sources'][:3], 1):\n",
        "    print(f\"\\n  {i}. {source[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Evaluation\n",
        "\n",
        "Evaluate retrieval quality and answer relevance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the RAG system\n",
        "if result.get(\"sources\"):\n",
        "    evaluation = comprehensive_evaluation(\n",
        "        query=question,\n",
        "        retrieved_texts=result[\"sources\"],\n",
        "        answer=result[\"answer\"]\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nQuery: {evaluation['query']}\")\n",
        "    print(f\"\\nRetrieval Evaluation:\")\n",
        "    print(f\"  Relevance Score: {evaluation['retrieval_evaluation']['relevance_score']:.3f}\")\n",
        "    print(f\"  Coverage: {evaluation['retrieval_evaluation']['coverage']:.3f}\")\n",
        "    print(f\"  Is Relevant: {evaluation['retrieval_evaluation']['is_relevant']}\")\n",
        "    print(f\"  Assessment: {evaluation['retrieval_evaluation']['assessment']}\")\n",
        "    \n",
        "    print(f\"\\nAnswer Evaluation:\")\n",
        "    print(f\"  Has Answer: {evaluation['answer_evaluation']['has_answer']}\")\n",
        "    print(f\"  Context Usage: {evaluation['answer_evaluation']['context_usage']:.3f}\")\n",
        "    print(f\"  Length Score: {evaluation['answer_evaluation']['length_score']:.3f}\")\n",
        "    print(f\"  Quality: {evaluation['answer_evaluation']['overall_quality']}\")\n",
        "    \n",
        "    print(f\"\\nOverall Score: {evaluation['overall_score']:.3f}\")\n",
        "    print(f\"Recommendation: {evaluation['recommendation']}\")\n",
        "else:\n",
        "    print(\"No sources available for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: ML Components\n",
        "\n",
        "### 7.1 Document Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test summarization\n",
        "if result.get(\"sources\"):\n",
        "    print(\"Original retrieved chunks:\")\n",
        "    for i, source in enumerate(result[\"sources\"][:2], 1):\n",
        "        print(f\"\\n{i}. {source[:200]}...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARIZED VERSION:\")\n",
        "    print(\"=\" * 60)\n",
        "    summary = summarize_chunks(result[\"sources\"])\n",
        "    print(summary)\n",
        "    \n",
        "    # Use summarization in RAG\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RAG WITH SUMMARIZATION:\")\n",
        "    print(\"=\" * 60)\n",
        "    result_with_summary = rag.ask(question, use_summarization=True)\n",
        "    print(result_with_summary[\"answer\"])\n",
        "else:\n",
        "    print(\"No sources available for summarization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Question Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test question classification\n",
        "test_questions = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"How does machine learning work?\",\n",
        "    \"Compare AI and traditional programming.\",\n",
        "    \"Is AI better than human intelligence?\",\n",
        "    \"When was the first computer invented?\"\n",
        "]\n",
        "\n",
        "print(\"Question Classification Results:\")\n",
        "print(\"=\" * 60)\n",
        "for q in test_questions:\n",
        "    q_type = classify_question_type(q)\n",
        "    print(f\"\\nQuestion: {q}\")\n",
        "    print(f\"Type: {q_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Interactive Question-Answering\n",
        "\n",
        "Try asking your own questions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Q&A\n",
        "def ask_question_interactive(question: str, use_summary: bool = False):\n",
        "    \"\"\"Helper function for interactive questioning.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    result = rag.ask(question, use_summarization=use_summary)\n",
        "    \n",
        "    print(\"ANSWER:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(result[\"answer\"])\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Sources: {result['num_sources']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example questions - modify these or add your own!\n",
        "questions = [\n",
        "    \"What are the key concepts discussed?\",\n",
        "    \"Can you summarize the main points?\",\n",
        "    \"What is the conclusion?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    ask_question_interactive(q)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated a complete RAG pipeline:\n",
        "\n",
        "1. ✅ **Data Ingestion**: Extracted text from PDFs\n",
        "2. ✅ **Preprocessing**: Cleaned and chunked documents\n",
        "3. ✅ **Embeddings**: Generated semantic embeddings\n",
        "4. ✅ **Vector Database**: Stored embeddings in ChromaDB\n",
        "5. ✅ **RAG Pipeline**: Implemented retrieval + LLM reasoning\n",
        "6. ✅ **Evaluation**: Assessed retrieval quality\n",
        "7. ✅ **ML Components**: Summarization and question classification\n",
        "\n",
        "### Key Takeaways:\n",
        "- RAG combines semantic search with LLM reasoning\n",
        "- Chunking strategy affects retrieval quality\n",
        "- Embedding models determine semantic understanding\n",
        "- Evaluation helps improve the system\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different chunk sizes\n",
        "- Try different embedding models\n",
        "- Add more documents to improve coverage\n",
        "- Fine-tune retrieval parameters\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
